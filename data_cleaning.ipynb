{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f56f0f",
   "metadata": {},
   "source": [
    "# DL Survival - Ventilation Outcomes\n",
    " Updated 21/11/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eaaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import miceforest as mf\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8194f0a",
   "metadata": {},
   "source": [
    "## 1. Data cleaning\n",
    "\n",
    "- Import MIMIC III data\n",
    "- Review column unique values, assign correct data types\n",
    "- Impute missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346094f4",
   "metadata": {},
   "source": [
    "### 1.1: Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb38842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mimic_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f101fe",
   "metadata": {},
   "source": [
    "#### 1.1.1: Column lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95780e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view and reorder columns\n",
    "cols = list(df.columns)\n",
    "new_cols = ['Unnamed: 0','hadm_id','subject_id','gender','ethnicity','marital_status','insurance','language','aortic','mit','tricuspid',\n",
    "            'pulmonary','cabg','temp','bg_temp','hr','spo2','rr','sbp','dbp','meanbp','weight','height','cardiac_index','pt','ptt',\n",
    "            'inr','inr_1','fibrinogen','hb','hematocrit','plts','wcc','lymphocytes','neutrophils','alp','ast','alt','ggt',\n",
    "            'bilirubin_indirect','bilirubin_direct','bilirubin_total','chloride','magnesium','potassium','crp','bleed_time','albumin',\n",
    "            'creatinine','free_calcium','sodium','bicarb','bun','hba1c','glucose','lactate','po2','pco2','baseexcess','ph','aado2',\n",
    "            'fio2','ffp','insulin','cryo','prbc','infection','ventrate','tidalvol','vent_array','reintubation','liver_severe','liver_mild',\n",
    "            'rheum','cvd','aids','ckd','copd','arrhythmia','pud','smoking','pvd','paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi',\n",
    "            'dementia','first_careunit','last_careunit','admission_location','admission_type','hospital_expire_flag','admittime',\n",
    "            'dischtime','intime','outtime','ext_time','reint_time','los','icustay_seq','deathtime','plt','diab_un','diab_cc',\n",
    "            'dtoutput','specimen','dod']\n",
    "\n",
    "ptinfo=['Unnamed: 0','hadm_id','subject_id']\n",
    "\n",
    "demographics=['gender','ethnicity','marital_status','insurance','language']\n",
    "\n",
    "proceduretype=['aortic','mit','tricuspid','pulmonary','cabg']\n",
    "\n",
    "vitals=['temp','bg_temp','hr','spo2','rr','sbp','dbp','meanbp','weight','height','cardiac_index']\n",
    "\n",
    "labs=['pt','ptt','inr','inr_1','fibrinogen','hb','hematocrit','plts','wcc','lymphocytes','neutrophils','alp','ast','alt','ggt',\n",
    "'bilirubin_indirect','bilirubin_direct','bilirubin_total','chloride','magnesium','potassium','crp','bleed_time',\n",
    "'albumin','creatinine','free_calcium','sodium','bicarb','bun','hba1c','glucose','lactate']\n",
    "\n",
    "bloodgases=['po2','pco2','baseexcess','ph','aado2','fio2']\n",
    "\n",
    "products=['ffp','insulin','cryo','prbc','infection']\n",
    "\n",
    "ventilation=['ventrate','tidalvol','vent_array','reintubation']\n",
    "\n",
    "comorbidities=['liver_severe','liver_mild','rheum','cvd','aids','ckd','copd','arrhythmia','pud','smoking','pvd',\n",
    "'paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi','dementia']\n",
    "\n",
    "adm_cat=['first_careunit','last_careunit','admission_location','admission_type','hospital_expire_flag']\n",
    "\n",
    "adm_num=['admittime','dischtime','intime','outtime','ext_time','reint_time','los','icustay_seq','deathtime']\n",
    "\n",
    "others=['plt','diab_un','diab_cc','dtoutput','specimen','dod']\n",
    "\n",
    "timeseries=[*vitals,*labs,*bloodgases,*products,*ventilation,'plt','dtoutput']\n",
    "timeseries = [i for i in timeseries if i not in ('weight','height','reintubation', 'infection', 'vent_array')]\n",
    "    \n",
    "timeseries_valuenames = {'cardiac_index':'ci',\n",
    "                         'plts':'bloodproduct',\n",
    "                         'ffp':'bloodproduct',\n",
    "                         'insulin':'amount',\n",
    "                         'cryo':'bloodproduct',\n",
    "                         'prbc':'bloodproduct',\n",
    "                         'dtoutput':'output'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[new_cols]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f2201",
   "metadata": {},
   "source": [
    "### 1.2: Cleaning data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def94863",
   "metadata": {},
   "source": [
    "#### 1.2.0: NaN assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dae754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('NaT',np.datetime64('NaT'))\n",
    "df = df.replace(['[]','NaN',np.datetime64('NaT')],np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c1b51",
   "metadata": {},
   "source": [
    "#### 1.2.1: Datetime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c51d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column types as datetime\n",
    "time_cols = ['admittime','dischtime','intime','outtime','reint_time','ext_time','deathtime']\n",
    "for col in time_cols:\n",
    "    df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#dod\n",
    "df['dod'] = pd.to_datetime(df['dod'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK FOR ROWS WHERE DEATHTIME < INTIME OR ADMITTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[time_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b3b66",
   "metadata": {},
   "source": [
    "#### 1.2.2: Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41125aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in demographics:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ethnicity\n",
    "df.replace({'ethnicity':\n",
    "                {'unknown': np.NaN,'UNKNOWN':np.NaN,'UNABLE TO OBTAIN':np.NaN,\n",
    "                'OTHER':'other','WHITE':'white','BLACK/AFRICAN AMERICAN':'black','ASIAN':'asian',\n",
    "                'HISPANIC/LATINO':'hispanic','AMERICAN INDIAN/ALASKA NATIVE':'native'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['ethnicity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab789eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#marital_status\n",
    "df.replace({'marital_status':\n",
    "                {'UNKNOWN (DEFAULT)': np.NaN\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['marital_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#language\n",
    "df.replace({'language':\n",
    "                {'ENGLISH':'ENGL','?':np.NaN\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['marital_status'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54efba3",
   "metadata": {},
   "source": [
    "#### 1.2.3: ✔Procedure type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60293cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in proceduretype:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b46f11",
   "metadata": {},
   "source": [
    "#### 1.2.4: **Vitals / Blood Gases / Products + infection / Ventilation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f47a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for Jahan/others\n",
    "# ventrate seems to be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833632bb",
   "metadata": {},
   "source": [
    "#### 1.2.5: ✔Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ea6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in comorbidities:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6159531",
   "metadata": {},
   "source": [
    "#### 1.2.6: Admissions (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d358bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in adm_cat:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_careunit\n",
    "df.replace({'first_careunit':\n",
    "                {'Cardiac Vascular Intensive Care Unit (CVICU)':'CVICU',\n",
    "                'Coronary Care Unit (CCU)':'CCU',\n",
    "                'Medical Intensive Care Unit (MICU)':'MICU',\n",
    "                'Surgical Intensive Care Unit (SICU)':'SICU',\n",
    "                'Neuro Intermediate':'Neuro Inter',\n",
    "                'Medical/Surgical Intensive Care Unit (MICU/SICU)':'MICU/SICU',\n",
    "                'Trauma SICU (TSICU)':'TSICU',\n",
    "                'Neuro Surgical Intensive Care Unit (Neuro SICU)':'Neuro SICU'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['first_careunit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_careunit\n",
    "df.replace({'last_careunit':\n",
    "                {'Cardiac Vascular Intensive Care Unit (CVICU)':'CVICU',\n",
    "                'Coronary Care Unit (CCU)':'CCU',\n",
    "                'Medical Intensive Care Unit (MICU)':'MICU',\n",
    "                'Surgical Intensive Care Unit (SICU)':'SICU',\n",
    "                'Neuro Intermediate':'Neuro Inter',\n",
    "                'Medical/Surgical Intensive Care Unit (MICU/SICU)':'MICU/SICU',\n",
    "                'Trauma SICU (TSICU)':'TSICU',\n",
    "                'Neuro Surgical Intensive Care Unit (Neuro SICU)':'Neuro SICU'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['last_careunit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4578e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#admission_location\n",
    "df.replace({'admission_location':\n",
    "                {'TRANSFER FROM HOSP/EXTRAM':'TRANSFER FROM HOSPITAL',\n",
    "                'PHYS REFERRAL/NORMAL DELI':'PHYSICIAN REFERRAL',\n",
    "                'TRANSFER FROM SKILLED NUR':'TRANSFER FROM SKILLED NURSING FACILITY',\n",
    "                'INFORMATION NOT AVAILABLE':np.NaN,\n",
    "                'CLINIC REFERRAL':'CLINIC REFERRAL/PREMATURE',\n",
    "                'EMERGENCY ROOM ADMIT':'EMERGENCY ROOM',\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['admission_location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a4471",
   "metadata": {},
   "source": [
    "#### 1.2.7: Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in others:\n",
    "#     print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe0ff5",
   "metadata": {},
   "source": [
    "### Parsing time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vent_array'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def va_parser(row, output=6):\n",
    "    \"\"\"\n",
    "    Takes row from `df` returns a list of starttime, endtime, vent duration \n",
    "    for first and (if applicable) second intubations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : row in df\n",
    "    output_ : select which output you want (use list index below) - e.g. args=[6] for all output when using df.apply()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    single list variable containing  \n",
    "        [0] int_time1: first intubation starttime\n",
    "        [1] ext_time1: first intubation endtime\n",
    "        [2] duration1: first intubation duration\n",
    "        [3] int_time2: second intubation starttime\n",
    "        [4] ext_time2: second intubation endtime \n",
    "        [5] duration2: second intubation duration\n",
    "        [6] all\n",
    "\n",
    "    \"\"\"\n",
    "    int_time1=np.NaN\n",
    "    ext_time1=np.NaN\n",
    "    duration1=np.NaN\n",
    "    int_time2=np.NaN\n",
    "    ext_time2=np.NaN\n",
    "    duration2=np.NaN\n",
    "    value = row['vent_array']\n",
    "    list=[]\n",
    "    '''a = value\n",
    "    print(value)'''\n",
    "    if value == np.NaN or pd.isna(value):\n",
    "        return np.NaN\n",
    "    a = value.replace(\"'\",'\"')\n",
    "    a = a.replace('\\n ...\\n',',').replace('\\n', ',').replace('...', '')\n",
    "    a = a.replace('datetime.','\"dt.')\n",
    "    a = a.replace('),', ')\",')\n",
    "    a = json.loads(a)\n",
    "    b = [(i['starttime'], i['endtime'], i['duration_hours']) for i in a]\n",
    "    int_time1=dt.datetime.strptime(b[0][0],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "    ext_time1=dt.datetime.strptime(b[0][1],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "    duration1=b[0][2]\n",
    "    \n",
    "    if output==0:\n",
    "        return int_time1\n",
    "    if output==1:\n",
    "        return ext_time1\n",
    "    if output==2:\n",
    "        return duration1\n",
    "\n",
    "    if len(b)>=2:\n",
    "        int_time2=dt.datetime.strptime(b[1][0],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "        ext_time2=dt.datetime.strptime(b[1][1],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "        duration2=b[1][2]\n",
    "    if output==3:\n",
    "        return int_time2\n",
    "    if output==4:\n",
    "        return ext_time2\n",
    "    if output==5:\n",
    "        return duration2\n",
    "    if output==6:\n",
    "        return int_time1, ext_time1, duration1, int_time2, ext_time2, duration2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_time1']=df.apply(va_parser, args=[0], axis=1)\n",
    "df['ext_time1']=df.apply(va_parser, args=[1], axis=1)\n",
    "df['duration1']=df.apply(va_parser, args=[2], axis=1)\n",
    "df['int_time2']=df.apply(va_parser, args=[3], axis=1)\n",
    "df['ext_time2']=df.apply(va_parser, args=[4], axis=1)\n",
    "df['duration2']=df.apply(va_parser, args=[5], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f27ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infection_parser(value, timelimit):\n",
    "    if value == np.NaN:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        a = value\n",
    "        a = a.replace('\\n ','')\n",
    "        a = a.replace('[','')\n",
    "        a = a.replace(']','')\n",
    "        a = a.replace(\"{'charttime': datetime.datetime\",'')\n",

    "        split = a.split('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_infection"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_parser2(value, timeDelta=None, timeLimits=None, valuename='value'):\n",
    "    # timeDelta is timedelta in hours from earliest entry\n",
    "    # timeLimits = (startTime, endTime)\n",
    "    # if both timeDelta and timeLimits are provided, timeDelta overrules.\n",
    "    # if both are None, then all timepoints are accepted\n",
    "    if value == np.NaN or pd.isna(value):\n",
    "        return np.NaN, np.NaN, np.NaN\n",
    "    a = value.replace(\"'\", '\"')\n",
    "    a = a.replace('\\n ...\\n',',').replace('\\n', ',').replace('...', '')\n",
    "    a = a.replace('datetime.', '\"dt.')\n",
    "    a = a.replace(f'), \"{valuename}\"', f')\", \"{valuename}\"')\n",
    "    a = a.replace('\"unit\": None', '\"unit\": \"None\"')\n",
    "    a = a.replace('starttime', 'charttime')\n",
    "    a = json.loads(a)\n",
    "    b = [(eval(i['charttime']), i[valuename]) for i in a]\n",
    "    \n",
    "    if timeDelta:\n",
    "        startTime = min(b, key=lambda x:x[0])[0]\n",
    "        inc_b = [i[1] for i in b if i[0] <= startTime + dt.timedelta(hours=timeDelta)]\n",
    "    else:\n",
    "        if timeLimits:\n",
    "            inc_b = [i[1] for i in b if i[0] >= timeLimits[0] and i[0] <= timeLimits[1]]\n",
    "        else:\n",
    "            inc_b = [i[1] for i in b]\n",
    "    if len(inc_b) == 0:\n",
    "        return np.NaN, np.NaN, np.NaN\n",
    "    \n",
    "    return sum(inc_b) / len(inc_b), max(inc_b), min(inc_b)\n",
    "\n",
    "# test_x = df[timeseries].iloc[0,0]\n",
    "# print(ts_parser(test_x,12))\n",
    "# print(ts_parser2(test_x, timeDelta=12))\n",
    "# print()\n",
    "# test_y = df['bg_temp'][9]\n",
    "# print(test_y)\n",
    "# print('Parser1: ', ts_parser(test_y, 36))\n",
    "# print('Parser2: ', ts_parser2(test_y, timeDelta=36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all rows where int_time1 OR ext_time1 are missing\n",
    "df = df[~(pd.isnull(df['int_time1']) | pd.isnull(df['ext_time1']))]\n",
    "df = df.reset_index()\n",
    "df = df.drop(axis=1, columns=['index', 'Unnamed: 0'], inplace=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in timeseries:\n",
    "    x = timeseries_valuenames[column] if column in timeseries_valuenames else \"value\"\n",
    "    meanList = []\n",
    "    maxList = []\n",
    "    minList = []\n",
    "    for i in df[column]:\n",
    "        y = ts_parser2(i, timeDelta=36, valuename=x)\n",
    "        meanList.append(y[0])\n",
    "        maxList.append(y[1])\n",
    "        minList.append(y[2])\n",
    "    df[column+'_mean']=meanList\n",
    "    df[column+'_max']=maxList\n",
    "    df[column+'_min']=minList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32333f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set limit and get list of variables missing above limit in `missing_cols`\n",
    "missing_limit = 50\n",
    "missing_cols = missing_data.loc[missing_data['% Missing Values']>missing_limit].index.tolist()\n",
    "print(missing_cols)\n",
    "missing_data = missing_data.loc[missing_data['% Missing Values']>missing_limit]\n",
    "missing_data = missing_data.sort_values(by=['% Missing Values'])\n",
    "pd.set_option('display.max_rows', None)\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22bd3c",
   "metadata": {},
   "source": [
    "### 1.3: Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72921c64",
   "metadata": {},
   "source": [
    
    "#### 1.3.0 Assessing for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [

    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98ae66",
   "metadata": {},
   "outputs": [],
   "source": [

    "# missing_data.loc[time_cols,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDroppedMissing = df[[i for i in df.columns if i not in missing_data.index]]\n",
    "print(list(dfDroppedMissing.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set limit and get list of variables missing above limit in `missing_cols`\n",
    "missing_limit = 20\n",
    "missing_cols = missing_data.loc[missing_data['% Missing Values']>missing_limit].index.tolist()\n",
    "print(missing_cols)\n",
    "missing_data.loc[missing_data['% Missing Values']>missing_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3: multiple imputation\n",
    "\n",
    "# x = missing_data.loc[missing_data['% Missing Values']> 0]\n",
    "# x.loc[[i for i in x.index if i not in time_cols],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc6f15",
   "metadata": {},
   "source": [
    "#### 1.3.1 Creating summary fields for time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761415b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that ts_parser2() works for the timeseries columns\n",
    "\n",
    "# for j in timeseries:\n",
    "#     for i in range(len(df[j])):\n",
    "#         try:\n",
    "#             if j in timeseries_valuenames:\n",
    "#                 ts_parser2(df[j][i], timeDelta=36, valuename=timeseries_valuenames[j])\n",
    "#             else:\n",
    "#                 ts_parser2(df[j][i], timeDelta=36)\n",
    "#         except\n",
    "#             print(j, i)\n",
    "#             break\n",
    "#     print(j, 'Fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f665d8",
   "metadata": {},
   "source": [
    "#### 1.3.2 Beginning imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDroppedMissing['int_time1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForImpute = pd.DataFrame([0 for i in range(dfDroppedMissing.shape[0])])\n",
    "\n",
    "# generating timeseries summary values\n",
    "for column in timeseries:\n",
    "    if column not in dfDroppedMissing.columns:\n",
    "        continue\n",
    "    x = timeseries_valuenames[column] if column in timeseries_valuenames else \"value\"\n",
    "    meanList = []\n",
    "    maxList = []\n",
    "    minList = []\n",
    "    for i in range(len(dfDroppedMissing[column])):\n",
    "        inTime = dfDroppedMissing['int_time1'][i].to_pydatetime()\n",
    "        y = ts_parser2(dfDroppedMissing[column][i], timeLimits=(inTime, inTime+dt.timedelta(hours=12)), valuename=x)\n",
    "        meanList.append(y[0])\n",
    "        maxList.append(y[1])\n",
    "        minList.append(y[2])\n",
    "    dfForImpute[column+'_mean'] = meanList\n",
    "    dfForImpute[column+'_max'] = maxList\n",
    "    dfForImpute[column+'_min'] = minList\n",
    "\n",
    "dfForImpute = dfForImpute[[i for i in dfForImpute.columns if i != 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add on non-time data for imputation\n",
    "dfForImpute = dfForImpute.copy()\n",
    "extraColumns = [i for i in dfDroppedMissing.columns if i not in list(dfForImpute.columns) + timeseries + ['infection', 'vent_array', 'int_time1', 'ext_time1'] + ptinfo + adm_num]\n",
    "print(extraColumns)\n",
    "for i in extraColumns:\n",
    "    if i in ('weight', 'height', 'duration1'):\n",
    "        dfForImpute[i] = df[i]\n",
    "    else:\n",
    "        dfForImpute[i] = df[i].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before imputation again\n",
    "dfForImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForImpute2 = dfForImpute[dfForImpute.columns[:]]\n",
    "\n",
    "kds = mf.ImputationKernel(\n",
    "  dfForImpute2,\n",
    "  datasets=1,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1991\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 3 iterations\n",
    "kds.mice(3)\n",
    "\n",
    "print(kds)\n",
    "\n",
    "dfImputed = kds.complete_data(dataset=0, inplace=False)\n",
    "print(dfImputed.isnull().sum(0))\n",
    "\n",
    "# after imputation\n",
    "dfImputed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray([i.to_pydatetime() for i in df[\"int_time1\"]])\n",
    "z = np.asarray([i.to_pydatetime() for i in df[\"outtime\"]])\n",
    "dfImputed['icu_stay_duration'] = [i.total_seconds() for i in z-y]\n",
    "dfImputed[['hadm_id','subject_id']] = df[['hadm_id','subject_id']]\n",
    "dfImputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding on critical rows that were removed when yeeting columns with high missing values\n",
    "\n",
    "for i in ('dod', 'deathtime'):\n",
    "    dfImputed[i] = df[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfImputed.to_csv('imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd547661",
   "metadata": {},
   "source": [
    "## 2.0: Exploring language and ventilation duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c083716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "dfImputed_onehot = dfImputed.copy()\n",
    "dfImputed1 = pd.get_dummies(dfImputed_onehot, columns = ['ethnicity', 'language'], prefix = ['eth', 'lang'])\n",
    "onehot_cols = [i for i in list(dfImputed1.columns) if i not in ['ethnicity', 'marital_status', 'language', 'admission_location']]\n",
    "dfImputed1['duration1'] = df['duration1']\n",
    "dfImputed1 = dfImputed1.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfImputed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from scipy.stats import kstest, norm, pearsonr, ttest_ind\n",
    "\n",
    "x = dfImputed1['lang_ENGL']\n",
    "y = df['duration1'].dropna(axis=0,how='any')\n",
    "\n",
    "#testing for normalcy\n",
    "mu, std = norm.fit(x)\n",
    "n = norm(loc=mu, scale=std)\n",
    "normalcy_p = kstest(x, n.cdf)[1]\n",
    "print('p-val: '+ str(normalcy_p))\n",
    "\n",
    "pyplot.scatter(x,y)\n",
    "covariance = np.cov(x,y)\n",
    "print('Covariance matrix: ')\n",
    "print(covariance)\n",
    "\n",
    "#find pearson R\n",
    "corr, _ = pearsonr(x,y)\n",
    "print('Pearsons R: %.3f' %corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "univariable = pd.DataFrame(index = ['eth_white','lang_ENGL'], columns = ['OR (95% CI)','p-value'])\n",
    "\n",
    "p_value = []\n",
    "OR = []\n",
    "time = []\n",
    "for column in ['lang_ENGL']:\n",
    "    for int_timelimit in range(0,48,1):\n",
    "        X = dfImputed1[column].values\n",
    "        X = sm.add_constant(X)\n",
    "        y = pd.cut(df['duration1'].dropna(how='any'),bins=[-0.1,int_timelimit,4000],labels=[0,1])\n",
    "        logit = sm.Logit(y, X)\n",
    "        model = logit.fit(method='bfgs',disp = False)\n",
    "        model_odds = pd.DataFrame(np.exp(model.params), columns=['OR'])\n",
    "        model_odds['p-value'] = model.pvalues\n",
    "        model_odds[['2.5%','97.5%']]= np.exp(model.conf_int())\n",
    "        model_odds.rename(index={'x1': column}, inplace = True)\n",
    "        # insert it into the dataframe 'univariable'\n",
    "        univariable.at[column,'OR (95% CI)'] = '{OR:.3f} ({lower:.3f} to {upper:.3f})'.format(OR = model_odds.at[column, 'OR'], \n",
    "                                                                                            lower = model_odds.at[column, '2.5%'], \n",
    "                                                                                            upper = model_odds.at[column, '97.5%'])\n",
    "        univariable.at[column,'p-value'] = '{0:.3f}'.format(model_odds.at[column, 'p-value'])\n",
    "        p_value.append(model_odds.at[column, 'p-value'])\n",
    "        OR.append(model_odds.at[column, 'OR'])\n",
    "        time.append(int_timelimit)\n",
    "\n",
    "univariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(time, OR, label='OR',color='blue')\n",
    "ax.set_xlabel(\"ventilation time\")\n",
    "ax.set_ylabel(\"OR\")\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(time, p_value, label='p-val',color='orange')\n",
    "ax2.set_ylabel(\"p-val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = dfImputed1['duration1'][dfImputed1['lang_ENGL']==1].dropna(axis=0,how='any')\n",
    "x2 = dfImputed1['duration1'][dfImputed1['lang_ENGL']==0].dropna(axis=0,how='any')\n",
    "plt.hist(x1,bins=[-0.1,2,4,6,10,13,16,20,25,30,40,50,60],range=(0,60))\n",
    "plt.hist(x2,bins=[-0.1,2,4,6,10,13,16,20,25,30,40,50,60],range=(0,60),color='orange')\n",
    "print('mean English: '+str(np.mean(x1)))\n",
    "print('mean non-English: '+str(np.mean(x2)))\n",
    "print('t_test p-val;: '+str(ttest_ind(x1, x2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd6b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84a701fa",
   "metadata": {},
   "source": [
    "## 2.1: DL Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['Unnamed: 0','hadm_id','subject_id','gender','ethnicity','marital_status','insurance','language','aortic','mit','tricuspid',\n",
    "            'pulmonary','cabg','temp','bg_temp','hr','spo2','rr','sbp','dbp','meanbp','weight','height','cardiac_index','pt','ptt',\n",
    "            'inr','inr_1','fibrinogen','hb','hematocrit','plts','wcc','lymphocytes','neutrophils','alp','ast','alt','ggt',\n",
    "            'bilirubin_indirect','bilirubin_direct','bilirubin_total','chloride','magnesium','potassium','crp','bleed_time','albumin',\n",
    "            'creatinine','free_calcium','sodium','bicarb','bun','hba1c','glucose','lactate','po2','pco2','baseexcess','ph','aado2',\n",
    "            'fio2','ffp','insulin','cryo','prbc','infection','ventrate','tidalvol','vent_array','reintubation','liver_severe','liver_mild',\n",
    "            'rheum','cvd','aids','ckd','copd','arrhythmia','pud','smoking','pvd','paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi',\n",
    "            'dementia','first_careunit','last_careunit','admission_location','admission_type','hospital_expire_flag','admittime',\n",
    "            'dischtime','intime','outtime','ext_time','reint_time','los','icustay_seq','deathtime','plt','diab_un','diab_cc',\n",
    "            'dtoutput','specimen','dod']\n",
    "\n",
    "# variables for analysis\n",
    "timeSeriesMMM=[]\n",
    "for column in timeseries:\n",
    "    for a in ['_mean','_max','_min']:\n",
    "        timeSeriesMMM.append(column+a)\n",
    "input_cols = ['gender','ethnicity','marital_status','insurance','language','aortic','mit','tricuspid',\n",
    "            'pulmonary','cabg','weight','height','liver_severe','liver_mild','rheum','cvd','aids','ckd','copd','arrhythmia',\n",
    "            'pud','smoking','pvd','paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi','dementia','first_careunit',\n",
    "            'admission_location','admission_type','diab_un','diab_cc',*timeSeriesMMM]\n",
    "outcome_cols = ['reintubation','hospital_expire_flag','los','duration1','duration2']\n",
    "others = ['admittime','dischtime','intime','outtime','ext_time','reint_time','deathtime','icustay_seq','dtoutput','specimen','dod',\n",
    "        'int_time1','ext_time1','int_time2','ext_time2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e98102",
   "metadata": {},
   "outputs": [],
   "source": [
    "templist=[]\n",
    "for column in timeseries:\n",
    "    for a in ['_mean','_max','_min']:\n",
    "        templist.append(column+a)\n",
    "print(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d57eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tridentenv",
   "language": "python",
   "name": "tridentenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
