{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Survival - Ventilation Outcomes\n",
    " Updated 21/11/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import miceforest as mf\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning\n",
    "\n",
    "- Import MIMIC III data\n",
    "- Review column unique values, assign correct data types\n",
    "- Impute missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mimic_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1: Column lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view and reorder columns\n",
    "cols = list(df.columns)\n",
    "new_cols = ['Unnamed: 0','hadm_id','subject_id','gender','ethnicity','marital_status','insurance','language','aortic','mit','tricuspid',\n",
    "            'pulmonary','cabg','temp','bg_temp','hr','spo2','rr','sbp','dbp','meanbp','weight','height','cardiac_index','pt','ptt',\n",
    "            'inr','inr_1','fibrinogen','hb','hematocrit','plts','wcc','lymphocytes','neutrophils','alp','ast','alt','ggt',\n",
    "            'bilirubin_indirect','bilirubin_direct','bilirubin_total','chloride','magnesium','potassium','crp','bleed_time','albumin',\n",
    "            'creatinine','free_calcium','sodium','bicarb','bun','hba1c','glucose','lactate','po2','pco2','baseexcess','ph','aado2',\n",
    "            'fio2','ffp','insulin','cryo','prbc','infection','ventrate','tidalvol','vent_array','reintubation','liver_severe','liver_mild',\n",
    "            'rheum','cvd','aids','ckd','copd','arrhythmia','pud','smoking','pvd','paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi',\n",
    "            'dementia','first_careunit','last_careunit','admission_location','admission_type','hospital_expire_flag','admittime',\n",
    "            'dischtime','intime','outtime','ext_time','reint_time','los','icustay_seq','deathtime','plt','diab_un','diab_cc',\n",
    "            'dtoutput','specimen','dod']\n",
    "\n",
    "ptinfo=['Unnamed:0','hadm_id','subject_id']\n",
    "\n",
    "demographics=['gender','ethnicity','marital_status','insurance','language']\n",
    "\n",
    "proceduretype=['aortic','mit','tricuspid','pulmonary','cabg']\n",
    "\n",
    "vitals=['temp','bg_temp','hr','spo2','rr','sbp','dbp','meanbp','weight','height','cardiac_index']\n",
    "\n",
    "labs=['pt','ptt','inr','inr_1','fibrinogen','hb','hematocrit','plts','wcc','lymphocytes','neutrophils','alp','ast','alt','ggt',\n",
    "'bilirubin_indirect','bilirubin_direct','bilirubin_total','chloride','magnesium','potassium','crp','bleed_time',\n",
    "'albumin','creatinine','free_calcium','sodium','bicarb','bun','hba1c','glucose','lactate']\n",
    "\n",
    "bloodgases=['po2','pco2','baseexcess','ph','aado2','fio2']\n",
    "\n",
    "products=['ffp','insulin','cryo','prbc','infection']\n",
    "\n",
    "ventilation=['ventrate','tidalvol','vent_array','reintubation']\n",
    "\n",
    "comorbidities=['liver_severe','liver_mild','rheum','cvd','aids','ckd','copd','arrhythmia','pud','smoking','pvd',\n",
    "'paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi','dementia']\n",
    "\n",
    "adm_cat=['first_careunit','last_careunit','admission_location','admission_type','hospital_expire_flag']\n",
    "\n",
    "adm_num=['admittime','dischtime','intime','outtime','ext_time','reint_time','los','icustay_seq','deathtime']\n",
    "\n",
    "others=['plt','diab_un','diab_cc','dtoutput','specimen','dod']\n",
    "\n",
    "timeseries=[*vitals,*labs,*bloodgases,*products,*ventilation,'plt','dtoutput']\n",
    "timeseries = [i for i in timeseries if i not in ('weight','height','reintubation', 'infection', 'vent_array')]\n",
    "    \n",
    "timeseries_valuenames = {'cardiac_index':'ci',\n",
    "                         'plts':'bloodproduct',\n",
    "                         'ffp':'bloodproduct',\n",
    "                         'insulin':'amount',\n",
    "                         'cryo':'bloodproduct',\n",
    "                         'prbc':'bloodproduct',\n",
    "                         'dtoutput':'output'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[new_cols]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Cleaning data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.0: NaN assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('NaT',np.datetime64('NaT'))\n",
    "df = df.replace(['[]','NaN',np.datetime64('NaT')],np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1: Datetime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column types as datetime\n",
    "time_cols = ['admittime','dischtime','intime','outtime','reint_time','ext_time','deathtime']\n",
    "for col in time_cols:\n",
    "    df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#dod\n",
    "df['dod'] = pd.to_datetime(df['dod'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK FOR ROWS WHERE DEATHTIME < INTIME OR ADMITTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[time_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2: Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in demographics:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ethnicity\n",
    "df.replace({'ethnicity':\n",
    "                {'unknown': np.NaN,'UNKNOWN':np.NaN,'UNABLE TO OBTAIN':np.NaN,\n",
    "                'OTHER':'other','WHITE':'white','BLACK/AFRICAN AMERICAN':'black','ASIAN':'asian',\n",
    "                'HISPANIC/LATINO':'hispanic','AMERICAN INDIAN/ALASKA NATIVE':'native'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['ethnicity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marital_status\n",
    "df.replace({'marital_status':\n",
    "                {'UNKNOWN (DEFAULT)': np.NaN\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['marital_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language\n",
    "df.replace({'language':\n",
    "                {'ENGLISH':'ENGL','?':np.NaN\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['marital_status'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3: ✔Procedure type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in proceduretype:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4: **Vitals / Blood Gases / Products + infection / Ventilation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for Jahan/others\n",
    "# ventrate seems to be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5: ✔Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in comorbidities:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6: Admissions (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in adm_cat:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_careunit\n",
    "df.replace({'first_careunit':\n",
    "                {'Cardiac Vascular Intensive Care Unit (CVICU)':'CVICU',\n",
    "                'Coronary Care Unit (CCU)':'CCU',\n",
    "                'Medical Intensive Care Unit (MICU)':'MICU',\n",
    "                'Surgical Intensive Care Unit (SICU)':'SICU',\n",
    "                'Neuro Intermediate':'Neuro Inter',\n",
    "                'Medical/Surgical Intensive Care Unit (MICU/SICU)':'MICU/SICU',\n",
    "                'Trauma SICU (TSICU)':'TSICU',\n",
    "                'Neuro Surgical Intensive Care Unit (Neuro SICU)':'Neuro SICU'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['first_careunit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_careunit\n",
    "df.replace({'last_careunit':\n",
    "                {'Cardiac Vascular Intensive Care Unit (CVICU)':'CVICU',\n",
    "                'Coronary Care Unit (CCU)':'CCU',\n",
    "                'Medical Intensive Care Unit (MICU)':'MICU',\n",
    "                'Surgical Intensive Care Unit (SICU)':'SICU',\n",
    "                'Neuro Intermediate':'Neuro Inter',\n",
    "                'Medical/Surgical Intensive Care Unit (MICU/SICU)':'MICU/SICU',\n",
    "                'Trauma SICU (TSICU)':'TSICU',\n",
    "                'Neuro Surgical Intensive Care Unit (Neuro SICU)':'Neuro SICU'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['last_careunit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#admission_location\n",
    "df.replace({'admission_location':\n",
    "                {'TRANSFER FROM HOSP/EXTRAM':'TRANSFER FROM HOSPITAL',\n",
    "                'PHYS REFERRAL/NORMAL DELI':'PHYSICIAN REFERRAL',\n",
    "                'TRANSFER FROM SKILLED NUR':'TRANSFER FROM SKILLED NURSING FACILITY',\n",
    "                'INFORMATION NOT AVAILABLE':np.NaN,\n",
    "                'CLINIC REFERRAL':'CLINIC REFERRAL/PREMATURE',\n",
    "                'EMERGENCY ROOM ADMIT':'EMERGENCY ROOM',\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['admission_location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7: Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in others:\n",
    "#     print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "id": "957680bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vent_array'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "def va_parser(row, output=6):\n",
    "    \"\"\"\n",
    "    Takes row index from `df` returns a list of starttime, endtime, vent duration \n",
    "    for first and (if applicable) second intubations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : row in df\n",
    "    output_ : select which output you want (use list index below) - e.g. args=[6] for all output when using df.apply()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    single list variable containing  \n",
    "        [0] int_time1: first intubation starttime\n",
    "        [1] ext_time1: first intubation endtime\n",
    "        [2] duration1: first intubation duration\n",
    "        [3] int_time2: second intubation starttime\n",
    "        [4] ext_time2: second intubation endtime \n",
    "        [5] duration2: second intubation duration\n",
    "        [6] all\n",
    "\n",
    "    \"\"\"\n",
    "    int_time1=np.NaN\n",
    "    ext_time1=np.NaN\n",
    "    duration1=np.NaN\n",
    "    int_time2=np.NaN\n",
    "    ext_time2=np.NaN\n",
    "    duration2=np.NaN\n",
    "    value = row['vent_array']\n",
    "    list=[]\n",
    "    '''a = value\n",
    "    print(value)'''\n",
    "    if value == np.NaN or pd.isna(value):\n",
    "        return np.NaN\n",
    "    a = value.replace(\"'\",'\"')\n",
    "    a = a.replace('\\n ...\\n',',').replace('\\n', ',').replace('...', '')\n",
    "    a = a.replace('datetime.','\"dt.')\n",
    "    a = a.replace('),', ')\",')\n",
    "    a = json.loads(a)\n",
    "    b = [(i['starttime'], i['endtime'], i['duration_hours']) for i in a]\n",
    "    int_time1=dt.datetime.strptime(b[0][0],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "    ext_time1=dt.datetime.strptime(b[0][1],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "    duration1=b[0][2]\n",
    "    \n",
    "    if output==0:\n",
    "        return int_time1\n",
    "    if output==1:\n",
    "        return ext_time1\n",
    "    if output==2:\n",
    "        return duration1\n",
    "\n",
    "    if len(b)>=2:\n",
    "        int_time2=dt.datetime.strptime(b[1][0],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "        ext_time2=dt.datetime.strptime(b[1][1],'dt.datetime(%Y, %m, %d, %H, %M)')\n",
    "        duration2=b[1][2]\n",
    "    if output==3:\n",
    "        return int_time2\n",
    "    if output==4:\n",
    "        return ext_time2\n",
    "    if output==5:\n",
    "        return duration2\n",
    "    if output==6:\n",
    "        return int_time1, ext_time1, duration1, int_time2, ext_time2, duration2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_time1']=df.apply(va_parser, args=[0], axis=1)\n",
    "df['ext_time1']=df.apply(va_parser, args=[1], axis=1)\n",
    "df['duration1']=df.apply(va_parser, args=[2], axis=1)\n",
    "df['int_time2']=df.apply(va_parser, args=[3], axis=1)\n",
    "df['ext_time2']=df.apply(va_parser, args=[4], axis=1)\n",
    "df['duration2']=df.apply(va_parser, args=[5], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infection_parser(value, timelimit):\n",
    "    if value == np.NaN:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        a = value\n",
    "        a = a.replace('\\n ','')\n",
    "        a = a.replace('[','')\n",
    "        a = a.replace(']','')\n",
    "        a = a.replace(\"{'charttime': datetime.datetime\",'')\n",
    "        split = a.split('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ead3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_parser(value, timelimit):\n",
    "    \"\"\"\n",
    "    Takes single string of timeseries data in MIMIC format and returns the mean, max, min values   \n",
    "    Parameters\n",
    "    ----------\n",
    "    value : single string of timeseries data in MIMIC format\n",
    "    timelimit : time (in hours) from the first data entry to include data up to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg : mean of all values within specified time period\n",
    "    max_: maximum of all values within specified time period\n",
    "    min_: minimum of all values within specified time period\n",
    "    \"\"\"\n",
    "    if value == np.NaN:\n",
    "        return np.NaN, np.NaN, np.NaN\n",
    "    else:\n",
    "        a = value\n",
    "        a = a.replace('\\n ','')\n",
    "        a = a.replace('[','')\n",
    "        a = a.replace(']','')\n",
    "        a = a.replace(\"{'charttime': datetime.datetime\",'')\n",
    "        split = a.split('}')\n",
    "        del split[-1]\n",
    "        times = []\n",
    "        values = []\n",
    "        for n in range(0,len(split)):\n",
    "            subsplit = split[n].split(\", 'value'\")\n",
    "            t = datetime.strptime(subsplit[0],'(%Y, %m, %d, %H, %M)')\n",
    "            times.append(t)\n",
    "            v = float(subsplit[1].replace(': ',''))\n",
    "            values.append(v)\n",
    "        starttime = times[0]\n",
    "        endtime = times[0] + timedelta(hours=timelimit)\n",
    "        #find the average\n",
    "        incl_values = []\n",
    "        for n in range(0,len(split)):\n",
    "            if times[n] > starttime and times[n] < endtime: \n",
    "                incl_values.append(values[n])\n",
    "        print(incl_values)\n",
    "        avg = statistics.mean(incl_values)\n",
    "        max_ = max(incl_values)\n",
    "        min_ = min(incl_values)\n",
    "        return avg, max_, min_\n",
    "\n",
    "def ts_parser2(value, timeDelta=None, timeLimits=None, valuename='value'):\n",
    "    # timeDelta is timedelta in hours from earliest entry\n",
    "    # timeLimits = (startTime, endTime)\n",
    "    # if both timeDelta and timeLimits are provided, timeDelta overrules.\n",
    "    # if both are None, then all timepoints are accepted\n",
    "    \n",
    "    if value == np.NaN or pd.isna(value):\n",
    "        return np.NaN, np.NaN, np.NaN\n",
    "    \n",
    "    a = value.replace(\"'\", '\"')\n",
    "    a = a.replace('\\n ...\\n',',').replace('\\n', ',').replace('...', '')\n",
    "    a = a.replace('datetime.', '\"dt.')\n",
    "    a = a.replace(f'), \"{valuename}\"', f')\", \"{valuename}\"')\n",
    "    a = a.replace('\"unit\": None', '\"unit\": \"None\"')\n",
    "    a = a.replace('starttime', 'charttime')\n",
    "    a = json.loads(a)\n",
    "    b = [(eval(i['charttime']), i[valuename]) for i in a]\n",
    "    \n",
    "    if timeDelta:\n",
    "        startTime = min(b, key=lambda x:x[0])[0]\n",
    "        inc_b = [i[1] for i in b if i[0] <= startTime + dt.timedelta(hours=timeDelta)]\n",
    "    else:\n",
    "        if timeLimits:\n",
    "            inc_b = [i[1] for i in b if i[0] >= timeLimits[0] and i[0] <= timeLimits[1]]\n",
    "        else:\n",
    "            inc_b = [i[1] for i in b]\n",
    "    return sum(inc_b) / len(inc_b), max(inc_b), min(inc_b)\n",
    "\n",
    "test_x = df[timeseries].iloc[0,0]\n",

    "print(ts_parser(test_x,12))\n",
    "print(ts_parser2(test_x, timeDelta=12))\n",
    "print()\n",
    "test_y = df['bg_temp'][9]\n",
    "print(test_y)\n",
    "print('Parser1: ', ts_parser(test_y, 36))\n",
    "print('Parser2: ', ts_parser2(test_y, timeDelta=36))"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.0 Assessing for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula for checking % missing values\n",
    "def missing_values_table(df): \n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns = {0: 'Missing Values', 1: '% Missing Values'})\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "missing_data = missing_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set limit and get list of variables missing above limit in `missing_cols`\n",
    "missing_limit = 20\n",
    "missing_cols = missing_data.loc[missing_data['% Missing Values']>missing_limit].index.tolist()\n",
    "print(missing_cols)\n",
    "missing_data.loc[missing_data['% Missing Values']>missing_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.loc[time_cols,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: delete all rows in `missing_cols` (set inplace to true to execute)\n",
    "\n",
    "df.drop(columns=missing_cols, inplace=False)\n",
    "print(list(df.columns))\n",
    "\n",
    "# reset index\n",
    "#df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: impute data based on median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3: multiple imputation\n",
    "\n",
    "x = missing_data.loc[missing_data['% Missing Values']> 0]\n",
    "x.loc[[i for i in x.index if i not in time_cols],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Creating summary fields for time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that ts_parser2() works for the timeseries columns\n",
    "\n",
    "# for j in timeseries:\n",
    "#     for i in range(len(df[j])):\n",
    "#         try:\n",
    "#             if j in timeseries_valuenames:\n",
    "#                 ts_parser2(df[j][i], timeDelta=36, valuename=timeseries_valuenames[j])\n",
    "#             else:\n",
    "#                 ts_parser2(df[j][i], timeDelta=36)\n",
    "#         except:\n",
    "#             print(j, i)\n",
    "#             break\n",
    "#     print(j, 'Fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Beginning imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = missing_data.loc[missing_data['% Missing Values']> 0]\n",
    "\n",
    "dfForImpute = df[[i for i in list(x.index) if i not in timeseries+['infection', 'vent_array']]]\n",
    "dfForImpute = df[['ethnicity', 'marital_status', 'language', 'admission_location']]\n",
    "dfForImpute\n",
    "for i in ['ethnicity', 'marital_status', 'language', 'admission_location']:\n",
    "    dfForImpute[i] = dfForImpute[i].astype('category')\n",
    "\n",
    "# before imputation\n",
    "dfForImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kds = mf.ImputationKernel(\n",
    "  dfForImpute,\n",
    "  datasets=1,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1991\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 3 iterations\n",
    "kds.mice(2)\n",
    "\n",
    "print(kds)\n",
    "\n",
    "dfImputed = kds.complete_data(dataset=0, inplace=False)\n",
    "print(dfImputed.isnull().sum(0))\n",
    "\n",
    "# after imputation\n",
    "dfImputed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tridentenv",
   "language": "python",
   "name": "tridentenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
