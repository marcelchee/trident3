{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Survival - Ventilation Outcomes\n",
    " Updated 21/11/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import miceforest as mf\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning\n",
    "\n",
    "- Import MIMIC III data\n",
    "- Review column unique values, assign correct data types\n",
    "- Impute missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mimic_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1: Column lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view and reorder columns\n",
    "cols = list(df.columns)\n",
    "new_cols = ['Unnamed: 0','hadm_id','subject_id','gender','ethnicity','marital_status','insurance','language','aortic','mit','tricuspid',\n",
    "            'pulmonary','cabg','temp','bg_temp','hr','spo2','rr','sbp','dbp','meanbp','weight','height','cardiac_index','pt','ptt',\n",
    "            'inr','inr_1','fibrinogen','hb','hematocrit','plts','wcc','lymphocytes','neutrophils','alp','ast','alt','ggt',\n",
    "            'bilirubin_indirect','bilirubin_direct','bilirubin_total','chloride','magnesium','potassium','crp','bleed_time','albumin',\n",
    "            'creatinine','free_calcium','sodium','bicarb','bun','hba1c','glucose','lactate','po2','pco2','baseexcess','ph','aado2',\n",
    "            'fio2','ffp','insulin','cryo','prbc','infection','ventrate','tidalvol','vent_array','reintubation','liver_severe','liver_mild',\n",
    "            'rheum','cvd','aids','ckd','copd','arrhythmia','pud','smoking','pvd','paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi',\n",
    "            'dementia','first_careunit','last_careunit','admission_location','admission_type','hospital_expire_flag','admittime',\n",
    "            'dischtime','intime','outtime','ext_time','reint_time','los','icustay_seq','deathtime','plt','diab_un','diab_cc',\n",
    "            'dtoutput','specimen','dod']\n",
    "\n",
    "ptinfo=['Unnamed:0','hadm_id','subject_id']\n",
    "\n",
    "demographics=['gender','ethnicity','marital_status','insurance','language']\n",
    "\n",
    "proceduretype=['aortic','mit','tricuspid','pulmonary','cabg']\n",
    "\n",
    "vitals=['temp','bg_temp','hr','spo2','rr','sbp','dbp','meanbp','weight','height','cardiac_index']\n",
    "\n",
    "labs=['pt','ptt','inr','inr_1','fibrinogen','hb','hematocrit','plts','wcc','lymphocytes','neutrophils','alp','ast','alt','ggt',\n",
    "'bilirubin_indirect','bilirubin_direct','bilirubin_total','chloride','magnesium','potassium','crp','bleed_time',\n",
    "'albumin','creatinine','free_calcium','sodium','bicarb','bun','hba1c','glucose','lactate']\n",
    "\n",
    "bloodgases=['po2','pco2','baseexcess','ph','aado2','fio2']\n",
    "\n",
    "products=['ffp','insulin','cryo','prbc','infection']\n",
    "\n",
    "ventilation=['ventrate','tidalvol','vent_array','reintubation']\n",
    "\n",
    "comorbidities=['liver_severe','liver_mild','rheum','cvd','aids','ckd','copd','arrhythmia','pud','smoking','pvd',\n",
    "'paraplegia','ccf','met_ca','t2dm','t1dm','malig','mi','dementia']\n",
    "\n",
    "adm_cat=['first_careunit','last_careunit','admission_location','admission_type','hospital_expire_flag']\n",
    "\n",
    "adm_num=['admittime','dischtime','intime','outtime','ext_time','reint_time','los','icustay_seq','deathtime']\n",
    "\n",
    "others=['plt','diab_un','diab_cc','dtoutput','specimen','dod']\n",
    "\n",
    "timeseries=[*vitals,*labs,*bloodgases,*products,*ventilation,'plt','dtoutput']\n",
    "timeseries = [i for i in timeseries if i not in ('weight','height','reintubation', 'infection', 'vent_array')]\n",
    "    \n",
    "timeseries_valuenames = {'cardiac_index':'ci',\n",
    "                         'plts':'bloodproduct',\n",
    "                         'ffp':'bloodproduct',\n",
    "                         'insulin':'amount',\n",
    "                         'cryo':'bloodproduct',\n",
    "                         'prbc':'bloodproduct',\n",
    "                         'dtoutput':'output'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[new_cols]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Cleaning data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.0: NaN assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('NaT',np.datetime64('NaT'))\n",
    "df = df.replace(['[]','NaN',np.datetime64('NaT')],np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1: Datetime columns\n",
    "+ add vent_duration column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column types as datetime\n",
    "time_cols = ['admittime','dischtime','intime','outtime','reint_time','ext_time','deathtime']\n",
    "for col in time_cols:\n",
    "    df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#dod\n",
    "df['dod'] = pd.to_datetime(df['dod'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for getting ventilation duration (1st ventilation)\n",
    "\n",
    "\n",
    "### NOTE: NEED TO EDIT FORMULA FOR VENT DURATION BASED ON JAHAN'S FORMULA ###\n",
    "\n",
    "\n",
    "def get_vent_duration(row):\n",
    "    time_s = (row['ext_time']-row['intime']).total_seconds()\n",
    "    if math.isnan(time_s):\n",
    "        time_s = (row['deathtime']-row['intime']).total_seconds()\n",
    "    time_min = time_s / 60\n",
    "    time_h = time_min / 60\n",
    "    return time_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ~NEW COLUMN~ for vent_duration\n",
    "df['vent_duration'] = df.apply(get_vent_duration, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK FOR ROWS WHERE DEATHTIME < INTIME OR ADMITTIME\n",
    "xtime_cols = ['ext_time','vent_duration','admittime','dischtime','intime','outtime','reint_time','deathtime','dod']\n",
    "df.loc[df['vent_duration'] < 0][xtime_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[xtime_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2: Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in demographics:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ethnicity\n",
    "df.replace({'ethnicity':\n",
    "                {'unknown': np.NaN,'UNKNOWN':np.NaN,'UNABLE TO OBTAIN':np.NaN,\n",
    "                'OTHER':'other','WHITE':'white','BLACK/AFRICAN AMERICAN':'black','ASIAN':'asian',\n",
    "                'HISPANIC/LATINO':'hispanic','AMERICAN INDIAN/ALASKA NATIVE':'native'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['ethnicity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marital_status\n",
    "df.replace({'marital_status':\n",
    "                {'UNKNOWN (DEFAULT)': np.NaN\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['marital_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language\n",
    "df.replace({'language':\n",
    "                {'ENGLISH':'ENGL','?':np.NaN\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['marital_status'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3: ✔Procedure type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in proceduretype:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4: **Vitals / Blood Gases / Products + infection / Ventilation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for Jahan/others\n",
    "# ventrate seems to be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5: ✔Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in comorbidities:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6: Admissions (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in adm_cat:\n",
    "    print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_careunit\n",
    "df.replace({'first_careunit':\n",
    "                {'Cardiac Vascular Intensive Care Unit (CVICU)':'CVICU',\n",
    "                'Coronary Care Unit (CCU)':'CCU',\n",
    "                'Medical Intensive Care Unit (MICU)':'MICU',\n",
    "                'Surgical Intensive Care Unit (SICU)':'SICU',\n",
    "                'Neuro Intermediate':'Neuro Inter',\n",
    "                'Medical/Surgical Intensive Care Unit (MICU/SICU)':'MICU/SICU',\n",
    "                'Trauma SICU (TSICU)':'TSICU',\n",
    "                'Neuro Surgical Intensive Care Unit (Neuro SICU)':'Neuro SICU'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['first_careunit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_careunit\n",
    "df.replace({'last_careunit':\n",
    "                {'Cardiac Vascular Intensive Care Unit (CVICU)':'CVICU',\n",
    "                'Coronary Care Unit (CCU)':'CCU',\n",
    "                'Medical Intensive Care Unit (MICU)':'MICU',\n",
    "                'Surgical Intensive Care Unit (SICU)':'SICU',\n",
    "                'Neuro Intermediate':'Neuro Inter',\n",
    "                'Medical/Surgical Intensive Care Unit (MICU/SICU)':'MICU/SICU',\n",
    "                'Trauma SICU (TSICU)':'TSICU',\n",
    "                'Neuro Surgical Intensive Care Unit (Neuro SICU)':'Neuro SICU'\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['last_careunit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#admission_location\n",
    "df.replace({'admission_location':\n",
    "                {'TRANSFER FROM HOSP/EXTRAM':'TRANSFER FROM HOSPITAL',\n",
    "                'PHYS REFERRAL/NORMAL DELI':'PHYSICIAN REFERRAL',\n",
    "                'TRANSFER FROM SKILLED NUR':'TRANSFER FROM SKILLED NURSING FACILITY',\n",
    "                'INFORMATION NOT AVAILABLE':np.NaN,\n",
    "                'CLINIC REFERRAL':'CLINIC REFERRAL/PREMATURE',\n",
    "                'EMERGENCY ROOM ADMIT':'EMERGENCY ROOM',\n",
    "                }\n",
    "            }, \n",
    "            inplace=True)\n",
    "print(df['admission_location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7: Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in others:\n",
    "#     print(x,': ',df[x].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['vent_array'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ventarray_parser(value):\n",
    "#     int_time1=np.NaN\n",
    "#     ext_time1=np.NaN\n",
    "#     int_time2=np.NaN\n",
    "#     ext_time2=np.NaN\n",
    "#     if value == np.NaN:\n",
    "#         return np\n",
    "#     else:\n",
    "#         a = value\n",
    "#         for i in ['\\n ',\n",
    "#                   '[',']',\n",
    "#                   \"{'starttime': datetime.datetime\",\n",
    "#                   \" 'endtime': datetime.datetime\",\n",
    "#                   \" 'duration_hours': \"]:\n",
    "#             a = a.replace(i,'')\n",
    "#         split = a.split('}')\n",
    "#         del split[-1]\n",
    "#         int_time1=np.NaN\n",
    "#         ext_time1=np.NaN\n",
    "#         int_time2=np.NaN\n",
    "#         ext_time2=np.NaN\n",
    "#         if len(split) == 1:\n",
    "#             pass\n",
    "#         elif len(split) == 2:\n",
    "#             pass\n",
    "#         else:\n",
    "#             raise ValueError(\"length of vent_array is wonky\")\n",
    "\n",
    "# test_x = df['vent_array'][13]\n",
    "# ventarray_parser(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infection_parser(value, timelimit):\n",
    "    if value == np.NaN:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        a = value\n",
    "        a = a.replace('\\n ','')\n",
    "        a = a.replace('[','')\n",
    "        a = a.replace(']','')\n",
    "        a = a.replace(\"{'charttime': datetime.datetime\",'')\n",
    "        split = a.split('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_parser(value, timelimit):\n",
    "    \"\"\"\n",
    "    Takes single string of timeseries data in MIMIC format and returns the mean, max, min values   \n",
    "    Parameters\n",
    "    ----------\n",
    "    value : single string of timeseries data in MIMIC format\n",
    "    timelimit : time (in hours) from the first data entry to include data up to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg : mean of all values within specified time period\n",
    "    max_: maximum of all values within specified time period\n",
    "    min_: minimum of all values within specified time period\n",
    "    \"\"\"\n",
    "    if value == np.NaN:\n",
    "        return np.NaN, np.NaN, np.NaN\n",
    "    else:\n",
    "        a = value\n",
    "        a = a.replace('\\n ','')\n",
    "        a = a.replace('[','')\n",
    "        a = a.replace(']','')\n",
    "        a = a.replace(\"{'charttime': datetime.datetime\",'')\n",
    "        split = a.split('}')\n",
    "        del split[-1]\n",
    "        times = []\n",
    "        values = []\n",
    "        for n in range(0,len(split)):\n",
    "            subsplit = split[n].split(\", 'value'\")\n",
    "            t = datetime.strptime(subsplit[0],'(%Y, %m, %d, %H, %M)')\n",
    "            times.append(t)\n",
    "            v = float(subsplit[1].replace(': ',''))\n",
    "            values.append(v)\n",
    "        starttime = times[0]\n",
    "        endtime = times[0] + timedelta(hours=timelimit)\n",
    "        #find the average\n",
    "        incl_values = []\n",
    "        for n in range(0,len(split)):\n",
    "            if times[n] > starttime and times[n] < endtime: \n",
    "                incl_values.append(values[n])\n",
    "        print(incl_values)\n",
    "        avg = statistics.mean(incl_values)\n",
    "        max_ = max(incl_values)\n",
    "        min_ = min(incl_values)\n",
    "        return avg, max_, min_\n",
    "\n",
    "def ts_parser2(value, timeDelta=None, timeLimits=None, valuename='value'):\n",
    "    # timeDelta is timedelta in hours from earliest entry\n",
    "    # timeLimits = (startTime, endTime)\n",
    "    # if both timeDelta and timeLimits are provided, timeDelta overrules.\n",
    "    # if both are None, then all timepoints are accepted\n",
    "    \n",
    "    if value == np.NaN or pd.isna(value):\n",
    "        return np.NaN, np.NaN, np.NaN\n",
    "    \n",
    "    a = value.replace(\"'\", '\"')\n",
    "    a = a.replace('\\n ...\\n',',').replace('\\n', ',').replace('...', '')\n",
    "    a = a.replace('datetime.', '\"dt.')\n",
    "    a = a.replace(f'), \"{valuename}\"', f')\", \"{valuename}\"')\n",
    "    a = a.replace('\"unit\": None', '\"unit\": \"None\"')\n",
    "    a = a.replace('starttime', 'charttime')\n",
    "    a = json.loads(a)\n",
    "    b = [(eval(i['charttime']), i[valuename]) for i in a]\n",
    "    \n",
    "    if timeDelta:\n",
    "        startTime = min(b, key=lambda x:x[0])[0]\n",
    "        inc_b = [i[1] for i in b if i[0] <= startTime + dt.timedelta(hours=timeDelta)]\n",
    "    else:\n",
    "        if timeLimits:\n",
    "            inc_b = [i[1] for i in b if i[0] >= timeLimits[0] and i[0] <= timeLimits[1]]\n",
    "        else:\n",
    "            inc_b = [i[1] for i in b]\n",
    "    return sum(inc_b) / len(inc_b), max(inc_b), min(inc_b)\n",
    "\n",
    "test_x = df[timeseries].iloc[0,0]\n",
    "print(ts_parser(test_x,12))\n",
    "print(ts_parser2(test_x, timeDelta=12))\n",
    "print()\n",
    "test_y = df['bg_temp'][9]\n",
    "print(test_y)\n",
    "print('Parser1: ', ts_parser(test_y, 36))\n",
    "print('Parser2: ', ts_parser2(test_y, timeDelta=36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.0 Assessing for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula for checking % missing values\n",
    "def missing_values_table(df): \n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns = {0: 'Missing Values', 1: '% Missing Values'})\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "missing_data = missing_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set limit and get list of variables missing above limit in `missing_cols`\n",
    "missing_limit = 20\n",
    "missing_cols = missing_data.loc[missing_data['% Missing Values']>missing_limit].index.tolist()\n",
    "print(missing_cols)\n",
    "missing_data.loc[missing_data['% Missing Values']>missing_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.loc[time_cols,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: delete all rows in `missing_cols` (set inplace to true to execute)\n",
    "\n",
    "df.drop(columns=missing_cols, inplace=False)\n",
    "print(list(df.columns))\n",
    "\n",
    "# reset index\n",
    "#df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: impute data based on median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3: multiple imputation\n",
    "\n",
    "x = missing_data.loc[missing_data['% Missing Values']> 0]\n",
    "x.loc[[i for i in x.index if i not in time_cols],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Creating summary fields for time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that ts_parser2() works for the timeseries columns\n",
    "\n",
    "# for j in timeseries:\n",
    "#     for i in range(len(df[j])):\n",
    "#         try:\n",
    "#             if j in timeseries_valuenames:\n",
    "#                 ts_parser2(df[j][i], timeDelta=36, valuename=timeseries_valuenames[j])\n",
    "#             else:\n",
    "#                 ts_parser2(df[j][i], timeDelta=36)\n",
    "#         except:\n",
    "#             print(j, i)\n",
    "#             break\n",
    "#     print(j, 'Fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Beginning imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = missing_data.loc[missing_data['% Missing Values']> 0]\n",
    "\n",
    "dfForImpute = df[[i for i in list(x.index) if i not in timeseries+['infection', 'vent_array']]]\n",
    "dfForImpute = df[['ethnicity', 'marital_status', 'language', 'admission_location']]\n",
    "dfForImpute\n",
    "for i in ['ethnicity', 'marital_status', 'language', 'admission_location']:\n",
    "    dfForImpute[i] = dfForImpute[i].astype('category')\n",
    "\n",
    "# before imputation\n",
    "dfForImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kds = mf.ImputationKernel(\n",
    "  dfForImpute,\n",
    "  datasets=1,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1991\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 3 iterations\n",
    "kds.mice(2)\n",
    "\n",
    "print(kds)\n",
    "\n",
    "dfImputed = kds.complete_data(dataset=0, inplace=False)\n",
    "print(dfImputed.isnull().sum(0))\n",
    "\n",
    "# after imputation\n",
    "dfImputed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tridentenv",
   "language": "python",
   "name": "tridentenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
